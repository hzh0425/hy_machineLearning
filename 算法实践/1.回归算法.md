## 一 单变量线性回归

https://blog.csdn.net/Jwenxue/article/details/107028893

### 1.1 模型表示

让我们通过一个例子来开始：这个例子是预测住房价格的，我们要使用一个数据集，数据集包含俄勒冈州波特兰市的住房价格。在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。那么，你可以做的一件事就是构建一个模型，也许是条直线，从这个数据模型上来看，也许你可以告诉你的朋友，他能以大约220000(美元)左右的价格卖掉这个房子。这就是监督学习算法的一个例子。

![img](D:\Typora\机器学习\每周任务\assets\8e76e65ca7098b74a2e9bc8e9577adfc-1594697235104.png)

它被称作监督学习是因为对于每个数据来说，我们给出了“正确的答案”，即告诉我们：根据我们的数据来说，房子实际的价格是多少，而且，更具体来说，这是一个回归问题。回归一词指的是，我们根据之前的数据预测出一个准确的输出值，对于这个例子就是价格，同时，还有另一种最常见的监督学习方式，叫做分类问题，当我们想要预测离散的输出值，例如，我们正在寻找癌症肿瘤，并想要确定肿瘤是良性的还是恶性的，这就是0/1离散输出的问题。更进一步来说，在监督学习中我们有一个数据集，这个数据集被称训练集.

下面的h 代表学习算法的解决方案或函数也称为假设（**hypothesis**）

![img](D:\Typora\机器学习\每周任务\assets\ad0718d6e5218be6e6fce9dc775a38e6-1594697253234.png)

这就是一个监督学习算法的工作方式，我们可以看到这里有我们的训练集里房屋价格 我们把它喂给我们的学习算法，学习算法的工作了，然后输出一个函数，通常表示为小写  表示。 代表**hypothesis**(**假设**)，表示一个函数，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此  根据输入的 值来得出  值， 值对应房子的价格 因此， 是一个从 到  的函数映射。

我将选择最初的使用规则代表**hypothesis**，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设，然后将我们要预测的房屋的尺寸作为输入变量输入给，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达 ？

一种可能的表达方式为：h(x)=θ+θ1x，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。



### 1.2 代价函数

我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是**建模误差**（**modeling error**）。

![img](D:\Typora\机器学习\每周任务\assets\6168b654649a0537c67df6f2454dc9ba-1594697360383.png)

我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数

![59469738257](D:\Typora\机器学习\每周任务\assets\1594697382575.png)

最小0,

我们绘制一个等高线图，三个坐标分别为 Θ0,θ1,j()

![img](D:\Typora\机器学习\每周任务\assets\27ee0db04705fb20fab4574bb03064ab-1594697431082.png)

则可以看出在三维空间中存在一个使得最小的点。

代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。

在后续课程中，我们还会谈论其他的代价函数，但我们刚刚讲的选择是对于大多数线性回归问题非常合理的。

也许这个函数有点抽象，可能你仍然不知道它的内涵，在接下来的几个视频里，我们要更进一步解释代价函数J的工作原理，并尝试更直观地解释它在计算什么，以及我们使用它的目的。

关于代价函数的值观理解:

![img](D:\Typora\机器学习\每周任务\assets\10ba90df2ada721cf1850ab668204dc9-1594697490673.png)

![img](D:\Typora\机器学习\每周任务\assets\2c9fe871ca411ba557e65ac15d55745d-1594697494691.png)

也即当所选的参数不同时,代价函数的值也随之变化,我们的目的就是找到一组能让代价函数最小的参数

### 1.3 代码实现线性回归

1）最小二乘法(least square method)

求解 w 和 b 是使损失函数最小化的过程，在统计中，称为线性回归模型的最小二乘“参数估计”(parameter estimation)。我们可以将 L(w,b) 分别对 w 和 b 求导，得到

![img](D:\Typora\机器学习\算法部分\assets\856725-20190303225511784-1188061678.png)

令上述两式为0，可得到 w 和 b 最优解的闭式(closed-form)解：

![img](D:\Typora\机器学习\算法部分\assets\856725-20190303225639407-822453102.png)



> 关于公式的推导:
>
> ![59472753046](D:\Typora\机器学习\每周任务\assets\1594727530465.png)
>
> ![59472754122](D:\Typora\机器学习\每周任务\assets\1594727541220.png)
>
> ![59472756605](D:\Typora\机器学习\每周任务\assets\1594727566055.png)



```
#损失函数
def cost(x,y,w,b):
    sum_cost=0
    for i in range(len(x)):
        lx=x[i]
        ly=y[i]
        sum_cost+=(w*lx+b-ly)**2
    return sum_cost/len(x)    
cost(x,y,1,1,)
#核心函数
def average(data):
    sum=0
    for i in range(len(data)):
        sum+=data[i]
    return sum/len(data)    

def fit(x,y):
    x_bar=average(x)
    y_bar=average(y)
    lly=0
    llx=0
    m=len(x)
    for i in range(m):
        lx=x[i]
        ly=y[i]
        lly+=ly*(lx-x_bar)
        llx+=lx**2
    w=lly/(llx-m*x_bar**2)
    b=y_bar-w*x_bar
    return w,b
    
```



### 1.4 梯度下降

梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数 的最小值。

梯度下降背后的思想是：开始时我们随机选择一个参数的组合，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（**local minimum**），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（**global minimum**），选择不同的初始参数组合，可能会找到不同的局部最小值。

![img](D:\Typora\机器学习\每周任务\assets\db48c81304317847870d486ba5bb2015-1594697639174.jpg)

想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。

批量梯度下降（**batch gradient descent**）算法的公式为：

![img](D:\Typora\机器学习\每周任务\assets\7da5a5f635b1eb552618556f1b4aac1a-1594697660432.png)

其中是a学习率（**learning rate**），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。



总结:核心思想就是迭代,每次更新以此参数,再用一组新的参数计算代价函数的值,让代价函数下降,因为梯度是

一个多元函数下降最快的地方.



### 1.5 代码实现线性回归梯度下降

> 公式推导:
>
> ![59469780552](D:\Typora\机器学习\每周任务\assets\1594697805522.png)
>
> 

```
#损失函数
def cost(x,y,w,b):
    sum=0
    for i in range(len(x)):
        sum+=(w*x[i]+b-y[i])**2
    return sum/len(x)
#计算w,b
def compute(x,y,w,b):
    sum_b=0
    sum_w=0
    for i in range(len(x)):
        sum_b+=(w*x[i]+b-y[i])
        sum_w+=(w*x[i]+b-y[i])*x[i]
    return sum_b/len(x),sum_w/len(x)    

#迭代计算w,b
def fit(x,y,w,b,iter,alpha):
    cost_list=[]
    for i in range(iter):
        c=cost(x,y,w,b)
        cost_list.append(c)
        j1,j2=compute(x,y,w,b)
        b=b-alpha*j1
        w=w-alpha*j2
    return w,b,cost_list     
    
#测试
w,b,cost_list=fit(x,y,0,0,10,0.0002)
pred_y=w*x+b
plt.scatter(x,y)
plt.plot(x,pred_y,c='r')
plt.show()
```

![59472978162](D:\Typora\机器学习\每周任务\assets\1594729781628.png)

不同学习率 代价函数的变化曲线:



```
plt.figure(figsize=(10,6))
alphas=[0.0001,0.0003,0.006,0.009,0.01,0.02]
for i in range(len(alphas)):
    w1,b1,cost_list1=fit(x,y,0,0,10,alphas[i])
    plt.subplot(2,3,i+1)
    plt.plot(cost_list1)
plt.show()    

```

![59472979485](D:\Typora\机器学习\每周任务\assets\1594729897821.png)

## 二 逻辑回归

### 2.1 概述

在分类问题中，你要预测的变量  是离散的值，我们将学习一种叫做逻辑回归 (**Logistic Regression**) 的算法，这是目前最流行使用最广泛的一种学习算法。

在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的。

![img](D:\Typora\机器学习\每周任务\assets\a77886a6eff0f20f9d909975bb69a7ab.png)

我们从二元的分类问题开始讨论。

我们将因变量(**dependent variable**)可能属于的两个类分别称为负向类（**negative class**）和正向类（**positive class**），则因变量 y∈(0,1)，其中 0 表示负向类，1 表示正向类。

![img](D:\Typora\机器学习\每周任务\assets\f86eacc2a74159c068e82ea267a752f7.png)

如果我们要用线性回归算法来解决一个分类问题，对于分类，y  取值为 0 或者1，但如果你使用的是线性回归,尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。

顺便说一下，逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 y取值离散的情况，如：1 0 0 1。

### 2.2 假说函数

在这段视频中，我要给你展示假设函数的表达式，也就是说，在分类问题中，要用什么样的函数来表示我们的假设。此前我们说过，希望我们的分类器的输出值在0和1之间，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在0和1之间。

回顾在一开始提到的乳腺癌分类问题，我们可以用线性回归的方法求出适合数据的一条直线：

![img](D:\Typora\机器学习\每周任务\assets\29c12ee079c079c6408ee032870b2683.jpg)

根据线性回归模型我们只能预测连续的值，然而对于分类问题，我们需要输出0或1，我们可以预测：

![59473433530](D:\Typora\机器学习\每周任务\assets\1594734335305.png)

对于上图所示的数据，这样的一个线性模型似乎能很好地完成分类任务。假使我们又观测到一个非常大尺寸的恶性肿瘤，将其作为实例加入到我们的训练集中来，这将使得我们获得一条新的直线。

![img](D:\Typora\机器学习\每周任务\assets\d027a0612664ea460247c8637b25e306.jpg)

这时，再使用0.5作为阀值来预测肿瘤是良性还是恶性便不合适了。可以看出，线性回归模型，因为其预测的值可以超越[0,1]的范围，并不适合解决这样的问题。

我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。 逻辑回归模型的假设是：

![59473440687](D:\Typora\机器学习\每周任务\assets\1594734406873.png)

![59473446554](D:\Typora\机器学习\每周任务\assets\1594734465547.png)

### 2.3 判定边界

![59473462051](D:\Typora\机器学习\每周任务\assets\1594734620518.png)

![59473477953](D:\Typora\机器学习\每周任务\assets\1594734779537.png)

![59473480584](D:\Typora\机器学习\每周任务\assets\1594734805844.png)

### 2.4 代价函数

![59473490788](D:\Typora\机器学习\每周任务\assets\1594734907889.png)

![59473528764](D:\Typora\机器学习\每周任务\assets\1594735287649.png)

python代码的实现:

```
xxxxxxxxxx
import numpy as np
    
def cost(theta, X, y):
    
  theta = np.matrix(theta)
  X = np.matrix(X)
  y = np.matrix(y)
  first = np.multiply(-y, np.log(sigmoid(X* theta.T)))
  second = np.multiply((1 - y), np.log(1 - sigmoid(X* theta.T)))
  return np.sum(first - second) / (len(X))
```

在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：

![59473552134](D:\Typora\机器学习\每周任务\assets\1594735521340.png)

![59473561994](D:\Typora\机器学习\每周任务\assets\1594735619941.png)

### 2.5 简化代价函数

![59473570314](D:\Typora\机器学习\每周任务\assets\1594735703148.png)

![59473573636](D:\Typora\机器学习\每周任务\assets\1594735736365.png)

### 2.6代码实现

> 1.定义digmoid,cost函数
>
> 2.迭代计算cost,dw,db,更新w,b
>
> 3.绘制曲线

```
#1.初始化模型参数
def initialize_params(dims):
    w=np.zeros((dims,1))
    b=0
    return w,b
#映射函数
def sigmoid(x):
    return 1/(1+np.exp(-x))
#cost函数
def cost(x,y,w,b):
    hy=sigmoid(np.dot(x,w)+b)
    m=len(x)
    c=-1/m*np.sum(y*np.log(hy)+(1-y)*np.log(1-hy))
    #x 装置后为(2,m),hy-y为(m,1)
    #注意矩阵乘法,所以先后的顺序不能变化
    dw=np.dot(x.T,hy-y)/m
    db=np.sum(hy-y)/m
    return c,dw,db
#梯度下降
def fit(x,y,alpha,iter):
    w,b=initialize_params(x.shape[1])
    cost_list=[]
    for i in range(iter):
        #计算dw,db,cost
        c,dw,db=cost(x,y,w,b)
        w=w-alpha*dw
        d=b-alpha*db
        if(i%100==0):
            cost_list.append(c)
            #print('epoch %d cost %f'%(i,cost))
    return cost_list,w,b
#预测函数
def predict(test_x,w,b):
    hy=sigmoid(np.dot(test_x,w)+b)
    for i in range(len(hy)):
        if hy[i]>0.5:
            hy[i]=1
        else:
            hy[i]=0
    return hy         
    


    #7.绘制模型决策的边界图形函数
def plot_logistic(x_train,y_train,w,b):
    n=x_train.shape[0]
    xcord1=[]
    ycord1=[]
    xcord2=[]
    ycord2=[]
    for i in range(n):
        if  y_train[i]==1:
            xcord1.append(x_train[i][0])
            ycord1.append(x_train[i][1])
        else:
            xcord2.append(x_train[i][0])
            ycord2.append(x_train[i][1])
    fig=plt.figure()
    ax=fig.add_subplot(111)
    ax.scatter(xcord1,ycord1,s=32,c='r')
    ax.scatter(xcord2,ycord2,s=32,c='g')
    x=np.arange(-1.5,3,0.1)
    y=-(w[0]*x+b)/w[1]
    ax.plot(x,y)
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.show()
plot_logistic(x_train,y_train,w,b)    
```

![59481472704](D:\Typora\机器学习\每周任务\assets\1594814727046.png)



## 三多元回归

### 3.1 多维特征

![59481519616](D:\Typora\机器学习\每周任务\assets\1594815196161.png)

![59481526436](D:\Typora\机器学习\每周任务\assets\1594815264362.png)

### 3.2 多变量梯度下降

![59481541987](D:\Typora\机器学习\每周任务\assets\1594815419874.png)



![59481546527](D:\Typora\机器学习\每周任务\assets\1594815465279.png)



### 3.3 特征缩放

![59481574325](D:\Typora\机器学习\每周任务\assets\1594815743256.png)

![59481575144](D:\Typora\机器学习\每周任务\assets\1594815751442.png)



### 3.4 代码实现

```
#损失函数
def cost(x,y,theta):
    m=len(x)
    sum=np.sum((np.dot(x,theta)-y)**2)
    return sum/(m*2)
#迭代计算
def fit(x,y,theta,alpha,iter):
    cost_list=[]
    m=len(x)
    for i in range(iter):
        c=cost(x,y,theta)
        dtheta=np.dot(x.T,np.dot(x,theta)-y)/m
        theta=theta-alpha*dtheta
        if i%100==0:
            cost_list.append(c)
    return cost_list,theta    
    
    
from sklearn.datasets.samples_generator import make_classification
from sklearn.model_selection import train_test_split
x,labels = make_classification(n_samples=500, n_features=5,
                               n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=2)
x_train,x_test,y_train,y_test=train_test_split(x,labels,train_size=0.7)
#记得要把y转化成(m,1),也即一列的二维数组!!!
y_train=y_train.reshape(-1,1)
y_test=y_test.reshape(-1,1)

    
#定义梯度下降绘图函数
def draw():
    alphas=[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.10]
    plt.figure(figsize=(20,10))
    for i in range(len(alphas)):
        theta=np.zeros((x.shape[1],1))
        cost_list,theta=fit(x_train,y_train,theta,alphas[i],1000)
        plt.subplot(2,5,i+1)
        plt.title(alphas[i])
        plt.plot(cost_list)
    plt.show()     
draw()
```

![59481872614](D:\Typora\机器学习\每周任务\assets\1594818726146.png)

> 结论:学习率在0.009左右最好

### 3.5 特征和多项式回归

![59482001508](D:\Typora\机器学习\每周任务\assets\1594820015085.png)

![59482002656](D:\Typora\机器学习\每周任务\assets\1594820026564.png)

### 3.6 正规方程

![59482067437](D:\Typora\机器学习\每周任务\assets\1594820674373.png)

![59482070682](D:\Typora\机器学习\每周任务\assets\1594820706820.png)

最后theta=(x.shape[0],1)也即各个参数的大小







![59482075169](D:\Typora\机器学习\每周任务\assets\1594820751699.png)

### 3.7 利用正规方程与多元回归进行比较

这里的fit是前文3.4中多元回归的代码,可以看到二者结过很相近,因此计算少数量的多元任务,

直接用正规方程求解即可,复杂的,数量多的再用相应的别的算法.

![59482115179](D:\Typora\机器学习\每周任务\assets\1594823187380.png)



## 四 正则化⭐⭐

### 4.1 过拟合问题

![59490075116](D:\Typora\机器学习\每周任务\assets\1594900751162.png)

![59490081570](D:\Typora\机器学习\每周任务\assets\1594900815704.png)



### 4.2 正则化参数

![59490094120](D:\Typora\机器学习\每周任务\assets\1594900941200.png)

![59490099642](D:\Typora\机器学习\每周任务\assets\1594900996422.png)

### 4.3 正则化线性回归(概念)

![59490132168](D:\Typora\机器学习\每周任务\assets\1594901321680.png)

![59490133346](D:\Typora\机器学习\每周任务\assets\1594901333463.png)



### 4.4 正则化逻辑回归(概念)

![59490154248](D:\Typora\机器学习\每周任务\assets\1594901542481.png)

![59490155056](D:\Typora\机器学习\每周任务\assets\1594901550565.png)

## 五 岭回归

### 5.1 参数推导

![59490244917](D:\Typora\机器学习\每周任务\assets\1594902449171.png)

![59490256968](D:\Typora\机器学习\每周任务\assets\1594902569681.png)

### 5.2 λ的选择

![59490304374](D:\Typora\机器学习\每周任务\assets\1594903043747.png)

![59490306946](D:\Typora\机器学习\每周任务\assets\1594903069460.png)

![59490307806](D:\Typora\机器学习\每周任务\assets\1594903078063.png)

#### 5.2.1岭迹法确定λ

![59490549252](D:\Typora\机器学习\每周任务\assets\1594905492526.png)

```
import pandas as pd
import numpy as np
from sklearn import model_selection
from sklearn.linear_model import Ridge
import matplotlib.pyplot as plt

data=pd.read_excel(r'C:\Users\Administrator\Desktop\diabetes.xlsx')
#拆分为训练集和测试集
predictors=data.columns[:-1]
x_train,x_test,y_train,y_test=model_selection.train_test_split(data[predictors],data.Y,
                                                               test_size=0.2,random_state=1234)
#构造不同的lambda值
Lambdas=np.logspace(-5,2,200)
#存放偏回归系数
ridge_cofficients=[]
for Lambda in Lambdas:
    ridge=Ridge(alpha=Lambda,normalize=True)
    ridge.fit(x_train,y_train)
    ridge_cofficients.append(ridge.coef_)

#绘制岭迹曲线
plt.rcParams['font.sans-serif']=['Microsoft YaHei']
plt.rcParams['axes.unicode_minus']=False
plt.style.use('ggplot')
plt.plot(Lambdas,ridge_cofficients)
#x轴做对数处理
plt.xscale('log')
plt.xlabel('Log(Lambda)')
plt.ylabel('Cofficients')
plt.show()
```

![59490551655](D:\Typora\机器学习\每周任务\assets\1594905516557.png)

可以看出在0.01附近,大部分b都趋于平缓

但是不可能每次都画图来计算!

#### 5.2.2交叉验证法确定λ

直接调用sklearn库,进行交叉验证,求出对于λ值则选择平均评分最优的λ\lambdaλ值。

```
import pandas as pd
import numpy as np
from sklearn import model_selection
from sklearn.linear_model import RidgeCV

data=pd.read_excel(r'C:\Users\Administrator\Desktop\diabetes.xlsx')
#拆分为训练集和测试集
predictors=data.columns[:-1]
x_train,x_test,y_train,y_test=model_selection.train_test_split(data[predictors],data.Y,
                                                               test_size=0.2,random_state=1234)
#构造不同的lambda值
Lambdas=np.logspace(-5,2,200)
#设置交叉验证的参数，使用均方误差评估
ridge_cv=RidgeCV(alphas=Lambdas,normalize=True,scoring='neg_mean_squared_error',cv=10)
ridge_cv.fit(x_train,y_train)
print(ridge_cv.alpha_)

```

![59490558624](D:\Typora\机器学习\每周任务\assets\1594905586243.png)

#### 5.2.3最优模型构建

先调用ridge_cv求出最优lambda,接着带入ridge模型

```
import pandas as pd
import numpy as np
from sklearn import model_selection
from sklearn.linear_model import Ridge,RidgeCV
from sklearn.metrics import mean_squared_error

data=pd.read_excel(r'C:\Users\Administrator\Desktop\diabetes.xlsx')
data=data.drop(['AGE','SEX'],axis=1)
#拆分为训练集和测试集
predictors=data.columns[:-1]
x_train,x_test,y_train,y_test=model_selection.train_test_split(data[predictors],data.Y,
                                                               test_size=0.2,random_state=1234)
#构造不同的lambda值
Lambdas=np.logspace(-5,2,200)
#设置交叉验证的参数，使用均方误差评估
ridge_cv=RidgeCV(alphas=Lambdas,normalize=True,scoring='neg_mean_squared_error',cv=10)
ridge_cv.fit(x_train,y_train)

#基于最佳lambda值建模
ridge=Ridge(alpha=ridge_cv.alpha_,normalize=True)
ridge.fit(x_train,y_train)
#打印回归系数
print(pd.Series(index=['Intercept']+x_train.columns.tolist(),
                data=[ridge.intercept_]+ridge.coef_.tolist()))

#模型评估
ridge_pred=ridge.predict(x_test)
#均方误差
MSE=mean_squared_error(y_test,ridge_pred)
print(MSE)

```

#### 5.2.4用公式直接求解theta,

> ```
> theta=np.ones((x_train.shape[1],1))
> I=np.identity(x_train.shape[1])
> lamd=0.0419870708444391
> theta=np.linalg.inv(x_train.T.dot(x_train)+lamd*I).dot(x_train.T).dot(y_train)
> theta
> ```



## -----------------------------------

## 机器学习实战第四章内容

## -----------------------------------

## 四 线性模型

### 4.1 单元回归

数据准备:

```
#为了更好地进行案例分析，现在生成一些近似线性关系的数据：
x = 2 * np.random.rand(100,1)
y = 4 + 3 * x + np.random.randn(100,1)
plt.plot(x,y,"b.")
plt.xlabel("$x_1$",fontsize=18)
plt.ylabel("$y$",rotation=0, fontsize=18)
plt.axis([0,2,0,15])
plt.show()
```

![59487083687](D:\Typora\机器学习\每周任务\assets\1594870836874.png)

正规方程求解参数:

注意:若要求截距b,要额外多加一列1,否则只会求θ

```
# 为了方便求解偏置b，在X中加入一列全1
x1=np.c_[np.ones((100,1)),x]
theta=np.linalg.inv(x1.T.dot(x1)).dot(x1.T).dot(y)
theta
---
array([[ 3.87525733],
       [10.4726371 ]])

#预测
x_new=np.array([[0],[2]])
x_new_b=np.c_[np.ones((2,1)),x_new]
y_predict=x_new_b.dot(theta)
plt.plot(x_new,y_predict)
plt.scatter(x,y,c='r')
plt.show()
```

![59487094041](D:\Typora\机器学习\每周任务\assets\1594870940413.png)

sklearn求解:

对于sklearn,则无需额外加一列1,会自动求解出b

```
#sklearn线性模型
from sklearn.linear_model import LinearRegression
lr_model=LinearRegression()
lr_model.fit(x,y)
lr_model.intercept_,lr_model.coef_
```

### 4.2 梯度下降

#### 批量梯度下降:

每次迭代的是所有的数据集

```
def plot_gradient(x,y,theta,alpha,theta_path,iter):
    m=len(x)
    for i in range(iter):
        if(i<10):
            y_pred=x_new_b.dot(theta)
            style = "b-" if i > 0 else "r--"
            plt.plot(x_new_b,y_pred,style)
        g=2/m*(x.dot(theta)-y).T.dot(x)
        theta=theta-alpha*g
        if theta_path is not None:
            theta_path.append(theta)
    plt.xlabel("x")
    plt.ylabel("y")
    plt.title(alpha)
alphas=[0.02,0.05,0.09,0.1,0.2,0.5]    
plt.figure(figsize=(20,10))
for i in range(len(alphas)):
    theta=np.ones((2,1))
    plt.subplot(2,3,i+1)
    plt.scatter(x,y,c='b')
    plot_gradient(x1,y,theta,alphas[i],None,iter=1000)
plt.tight_layout()
plt.show() 
```

![59487107686](D:\Typora\机器学习\每周任务\assets\1594871076869.png)

#### 随机梯度下降

每次抽取一份样本进行迭代更新

```
theta_path_sgd = []
m = len(X_b)
np.random.seed(42)
 
n_epochs = 50
t0, t1 = 5,50
 
 
def learning_schedule(t):
    return t0/(t+t1)
 
theta = np.random.randn(2,1)
for epoch in range(n_epochs):
    for i in range(m):
        if epoch == 0 and i < 20:
            y_predict = X_new_b.dot(theta)
            style = "b-" if i > 0 else "r--"
            plt.plot(X_new, y_predict, style)
        random_index = np.random.randint(m)
        xi = X_b[random_index:random_index+1]
        yi = y[random_index:random_index+1]
        gradients = 2*xi.T.dot(xi.dot(theta)-yi)
        eta = learning_schedule(epoch*m+i)
        theta = theta - eta * gradients
        theta_path_sgd.append(theta)
        
plt.plot(X,y,"b.")
plt.xlabel("$x_1$",fontsize=18)
plt.ylabel("$y$",rotation=0,fontsize=18)
plt.axis([0,2,0,15])
plt.show()
```

![59487113156](D:\Typora\机器学习\每周任务\assets\1594871131565.png)

sklearn:

```
from sklearn.linear_model import SGDRegressor
 
sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)
sgd_reg.fit(X,y.ravel())
sgd_reg.intercept_, sgd_reg.coef_
(array([4.01500975]), array([2.9432024]))
```



#### 批量梯度下降

每次抽取一部分进行更新

```
theta_path_mgd = []
n_iternations = 50
minibatch_size = 20
 
np.random.seed(42)
theta = np.random.randn(2,1)
 
t0, t1 = 200, 1000
def learning_schedule(t):
    return t0/(t+t1)
 
t = 0
for epoch in range(n_iternations):
    shuffled_indices = np.random.permutation(m)
    X_b_shuffled = X_b[shuffled_indices]
    y_shuffled = y[shuffled_indices]
    for i in range(0,m,minibatch_size):
        t += 1
        xi = X_b_shuffled[i:i+minibatch_size]
        yi = y_shuffled[i:i+minibatch_size]
        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta)-yi)
        theta = theta - eta * gradients
        theta_path_mgd.append(theta)
 
theta
```

#### 三种下降法对比

```
theta_list = [theta_path_bgd, theta_path_mgd, theta_path_sgd]
theta_type = ["Batch","Mini-batch","Stochastic"]
theta_style = ["b-o","g-+","r-s"]
  
plt.figure(figsize=(8,5))
for i in reversed(range(3)):
    plt.plot(np.array(theta_list[i])[:,0],np.array(theta_list[i])[:,1],theta_style[i],
             linewidth=2,label=theta_type[i])
plt.xlabel(r"$\theta_0$",fontsize=20)
plt.ylabel(r"$\theta_1$",fontsize=20, rotation=0)
plt.axis([2.5,4.5,2.3,3.9])
plt.legend(fontsize=16,loc="upper left")
plt.show()
```

![59487120607](D:\Typora\机器学习\每周任务\assets\1594871206076.png)

> 从上图中可以看出，批量梯度下降最终能比较准确定位到损失函数的最优解，而随机梯度下降和小批量随机梯度下降都在最优解附近徘徊，并且随机梯度下降的参数随机摆动性更大。

### 4.3 多项式回归

#### Polynomialfeaturesasz解释

![59487134240](D:\Typora\机器学习\每周任务\assets\1594871342408.png)

#### 直线模型的问题:

```
np.random.seed(42)
 
m = 100
X = 6 * np.random.rand(m,1)-3
y = 0.5* X**2 + X + 2 + np.random.randn(m,1)
 
plt.plot(X,y,"b.")
plt.xlabel("$x_1$",fontsize=18)
plt.ylabel("$y$",fontsize=18, rotation=0)
 
plt.show()
```

![59487137693](D:\Typora\机器学习\每周任务\assets\1594871376937.png)

很明显一条直线无法很好的拟合,所以应选择多项式回归

#### 解决方法

> 将多项式回归中的高次方,利用sklearn的Polynomialfeaturesasz,进行处理,
>
> 转化成多元线性回归解决
>
> ![59487175718](D:\Typora\机器学习\每周任务\assets\1594871757181.png)

```
from sklearn.preprocessing import PolynomialFeatures
 
poly_features = PolynomialFeatures(degree=2, include_bias=False)
x_poly = poly_features.fit_transform(x)
#正规方程版本
#x_poly=np.c_[np.ones((len(x_poly),1)),x_poly]
#theta=np.linalg.inv(x_poly.T.dot(x_poly)).dot(x_poly.T).dot(y)
#print(theta)
#sklearn版本
from sklearn.linear_model import LinearRegression
lr_mode11=LinearRegression()
lr_mode11.fit(x_poly,y)
print(lr_mode11.intercept_,lr_mode11.coef_)
#绘图
x_new= np.linspace(-3,3,100).reshape(-1,1)
x_new_poly=poly_features.fit_transform(x_new)
y_new=lr_mode11.predict(x_new_poly)
plt.plot(x_new,y_new,'r-')
plt.plot(x,y,'b.')
plt.show()
```

![59487157603](D:\Typora\机器学习\每周任务\assets\1594871576030.png)

